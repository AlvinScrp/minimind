{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlvinScrp/minimind/blob/master/%E6%B5%8B%E8%AF%95%E4%BB%A3%E7%A0%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoPE"
      ],
      "metadata": {
        "id": "hqMXJqPK2ImE"
      },
      "id": "hqMXJqPK2ImE"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 函数 1: 预计算 RoPE 复数值\n",
        "# (来自您的第一个问题)\n",
        "# -----------------------------------------------------------------\n",
        "def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):\n",
        "    \"\"\"\n",
        "    预计算旋转位置编码（Rotary Position Embeddings, RoPE）所需的复数值\n",
        "\n",
        "    参数:\n",
        "        dim: 隐藏维度大小 (head_dim)\n",
        "        end: 最大序列长度，默认为32K\n",
        "        theta: RoPE中的缩放因子\n",
        "    返回:\n",
        "        pos_cis: 预计算好的复数形式的位置编码，形状为[end, dim//2]\n",
        "    \"\"\"\n",
        "    # 计算不同频率的逆频率项\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    # 生成位置索引\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
        "    # 计算外积得到每个位置对应的每个频率 (角度)\n",
        "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
        "    # 使用欧拉公式 e^(i*θ) = cos(θ) + i*sin(θ) 生成复数\n",
        "    # 幅值为1，相位为freqs的复数值\n",
        "    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return pos_cis\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# 函数 2: 应用 RoPE\n",
        "# (来自您的第二个问题)\n",
        "# -----------------------------------------------------------------\n",
        "def apply_rotary_emb(xq, xk, pos_cis):\n",
        "    \"\"\"\n",
        "    将旋转位置编码应用到查询(Q)和键(K)张量上\n",
        "\n",
        "    参数:\n",
        "        xq: 查询张量, 形状为[batch_size, seq_len, n_heads, head_dim]\n",
        "        xk: 键张量, 形状为[batch_size, seq_len, n_kv_heads, head_dim]\n",
        "        pos_cis: 预计算的位置编码复数, 形状为 [seq_len, head_dim // 2]\n",
        "\n",
        "    返回:\n",
        "        应用位置编码后的查询和键张量\n",
        "    \"\"\"\n",
        "    def unite_shape(pos_cis, x):\n",
        "        \"\"\"\n",
        "        调整pos_cis的形状使其与输入张量x兼容，便于广播计算\n",
        "        \"\"\"\n",
        "        ndim = x.ndim\n",
        "        # 预期的 pos_cis 形状 [seq_len, dim//2]\n",
        "        # 预期的 x 形状 [B, seq_len, H, dim//2]\n",
        "        assert 0 <= 1 < ndim\n",
        "        # 检查 pos_cis 的形状是否与 x 的 seq_len 和 dim//2 维度匹配\n",
        "        assert pos_cis.shape == (x.shape[1], x.shape[-1])\n",
        "        # 创建一个新形状，只保留序列长度和特征维度，其余维度设为1\n",
        "        # [1, seq_len, 1, dim//2]\n",
        "        shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "        return pos_cis.view(*shape)\n",
        "\n",
        "    # 将Q和K重塑并转换为复数形式\n",
        "    # [B, L, H, D] -> [B, L, H, D//2, 2]\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "\n",
        "    # 调整pos_cis的形状以便与输入张量兼容\n",
        "    # [L, D//2] -> [1, L, 1, D//2]\n",
        "    pos_cis = unite_shape(pos_cis, xq_)\n",
        "\n",
        "    # 应用旋转操作：在复数域中，乘以pos_cis等同于旋转\n",
        "    # (B, L, H, D//2) * (1, L, 1, D//2) -> (B, L, H, D//2)\n",
        "    xq_rotated_complex = xq_ * pos_cis\n",
        "    xk_rotated_complex = xk_ * pos_cis\n",
        "\n",
        "    # 转换回复数对应的实数对\n",
        "    # [B, L, H, D//2] -> [B, L, H, D//2, 2]\n",
        "    xq_out = torch.view_as_real(xq_rotated_complex)\n",
        "    xk_out = torch.view_as_real(xk_rotated_complex)\n",
        "\n",
        "    # 拍平最后一个维度，恢复 [B, L, H, D]\n",
        "    xq_out = xq_out.flatten(start_dim=3)\n",
        "    xk_out = xk_out.flatten(start_dim=3)\n",
        "\n",
        "    # 转换回输入张量的原始数据类型\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
        "\n"
      ],
      "metadata": {
        "id": "iH9U_Sg_2F2w"
      },
      "id": "iH9U_Sg_2F2w",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. 设置超参数 ---\n",
        "torch.manual_seed(42)  # 为了可复现性\n",
        "\n",
        "batch_size = 1\n",
        "seq_len = 4       # 序列长度\n",
        "n_heads = 2       # 查询头数量\n",
        "n_kv_heads = 1    # 键/值头数量 (模拟 GQA)\n",
        "head_dim = 8      # 头部维度 (必须是偶数)\n",
        "max_seq_len = 128 # 预计算的最大长度\n",
        "\n",
        "print(f\"--- Demo 参数 ---\")\n",
        "print(f\"Batch Size: {batch_size}, Seq Len: {seq_len}, Head Dim: {head_dim}\")\n",
        "print(f\"Query Heads: {n_heads}, KV Heads: {n_kv_heads}\\n\")\n",
        "# --- 2. 预计算 pos_cis ---\n",
        "# precompute_pos_cis 返回 [max_seq_len, head_dim // 2]\n",
        "pos_cis_precomputed = precompute_pos_cis(head_dim, max_seq_len)\n",
        "\n",
        "# 在实际使用中，我们只截取当前序列长度所需的部分\n",
        "# 形状变为 [seq_len, head_dim // 2] -> [4, 4]\n",
        "pos_cis_input = pos_cis_precomputed[:seq_len]\n",
        "\n",
        "print(f\"--- 预计算 pos_cis ---\")\n",
        "print(f\"预计算的 pos_cis 形状 (用于输入): {pos_cis_input.shape}\\n\")\n"
      ],
      "metadata": {
        "id": "q3PsrpuE3Bdl",
        "outputId": "e685da34-d150-4e31-9c80-74223acb0b5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q3PsrpuE3Bdl",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Demo 参数 ---\n",
            "Batch Size: 1, Seq Len: 4, Head Dim: 8\n",
            "Query Heads: 2, KV Heads: 1\n",
            "\n",
            "--- 预计算 pos_cis ---\n",
            "预计算的 pos_cis 形状 (用于输入): torch.Size([4, 4])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. 创建模拟的 Q 和 K 张量 ---\n",
        "xq = torch.randn(batch_size, seq_len, n_heads, head_dim)\n",
        "xk = torch.randn(batch_size, seq_len, n_kv_heads, head_dim)\n",
        "\n",
        "print(f\"--- 原始张量 (Batch 0, Head 0) ---\")\n",
        "print(\"原始 XQ (前4维):\")\n",
        "print(xq[0, :, 0, :4])\n",
        "# --- 4. 应用 RoPE 旋转 ---\n",
        "xq_rotated, xk_rotated = apply_rotary_emb(xq, xk, pos_cis_input)\n",
        "print(f\"\\n--- 旋转后张量 (Batch 0, Head 0) ---\")\n",
        "print(\"旋转后 XQ (前4维):\")\n",
        "print(xq_rotated[0, :, 0, :4])\n",
        "print(\"-> 可以看到数值已经完全不同了\\n\")\n"
      ],
      "metadata": {
        "id": "hB_qReVd3IUk",
        "outputId": "6e8fc905-e06d-4bc2-b795-887e36f378b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hB_qReVd3IUk",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 原始张量 (Batch 0, Head 0) ---\n",
            "原始 XQ (前4维):\n",
            "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
            "        [ 1.6423, -0.1596, -0.4974,  0.4396],\n",
            "        [-1.3847, -0.8712, -0.2234,  1.7174],\n",
            "        [-0.9138, -0.6581,  0.0780,  0.5258]])\n",
            "\n",
            "--- 旋转后张量 (Batch 0, Head 0) ---\n",
            "旋转后 XQ (前4维):\n",
            "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
            "        [ 1.0216,  1.2957, -0.5110,  0.4236],\n",
            "        [ 1.3684, -0.8965, -0.3315,  1.6998],\n",
            "        [ 0.9976,  0.5226,  0.0279,  0.5308]])\n",
            "-> 可以看到数值已经完全不同了\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. 验证 RoPE 的核心特性 ---\n",
        "# 验证 1: 范数 (Norm) 保持不变\n",
        "# RoPE 是纯旋转，不应改变向量的长度\n",
        "\n",
        "# 将 [B, L, H, D] 视为 [B, L, H, D//2, 2]\n",
        "xq_pairs_before = xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
        "xq_pairs_after = xq_rotated.float().reshape(*xq_rotated.shape[:-1], -1, 2)\n",
        "# 计算每对 (x, y) 的L2范数: sqrt(x^2 + y^2)\n",
        "norm_before = xq_pairs_before.norm(dim=-1)\n",
        "norm_after = xq_pairs_after.norm(dim=-1)\n",
        "print(\"--- 验证 1: 范数 (Norm) ---\")\n",
        "print(\"旋转前 XQ 范数 (Head 0, Pos 0):\")\n",
        "print(norm_before[0, 0, 0, :])\n",
        "print(\"旋转后 XQ 范数 (Head 0, Pos 0):\")\n",
        "print(norm_after[0, 0, 0, :])\n",
        "print(f\"范数是否保持不变? {torch.allclose(norm_before, norm_after, atol=1e-6)}\")\n",
        "\n",
        "# 验证 2: 点积 (Dot Product) 发生变化\n",
        "# 旋转改变了向量间的相对角度，因此点积会变\n",
        "\n",
        "# 取 Q 在位置 0 和 K 在位置 1 (不同位置)\n",
        "q0 = xq[0, 0, 0, :]       # B=0, L=0, H=0\n",
        "k1 = xk[0, 1, 0, :]       # B=0, L=1, H=0 (KV头会广播)\n",
        "q0_rotated = xq_rotated[0, 0, 0, :]\n",
        "k1_rotated = xk_rotated[0, 1, 0, :]\n",
        "dot_before = torch.dot(q0, k1)\n",
        "dot_after = torch.dot(q0_rotated, k1_rotated)\n",
        "\n",
        "print(f\"\\n--- 验证 2: 点积 (Dot Product) ---\")\n",
        "print(f\"Q[pos=0] · K[pos=1] (旋转前): {dot_before.item():.4f}\")\n",
        "print(f\"Q[pos=0] · K[pos=1] (旋转后): {dot_after.item():.4f}\")\n",
        "print(f\"点积是否发生变化? {not torch.allclose(dot_before, dot_after)}\")"
      ],
      "metadata": {
        "id": "BCAAKdkJ3Lej"
      },
      "id": "BCAAKdkJ3Lej",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}